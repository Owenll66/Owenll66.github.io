<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>A succinct solution for building decision stumps | Owenll66’s blog site</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="A succinct solution for building decision stumps" />
<meta name="author" content="owenll66" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Decision stumps are simple and good weak learners which could be applied in Boosting algorithms. This blog provides both matlab code and python for building decision stumps." />
<meta property="og:description" content="Decision stumps are simple and good weak learners which could be applied in Boosting algorithms. This blog provides both matlab code and python for building decision stumps." />
<link rel="canonical" href="http://localhost:4000/2019/04/14/decisionStump.html" />
<meta property="og:url" content="http://localhost:4000/2019/04/14/decisionStump.html" />
<meta property="og:site_name" content="Owenll66’s blog site" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-14T00:00:00+09:30" />
<script type="application/ld+json">
{"description":"Decision stumps are simple and good weak learners which could be applied in Boosting algorithms. This blog provides both matlab code and python for building decision stumps.","author":{"@type":"Person","name":"owenll66"},"@type":"BlogPosting","url":"http://localhost:4000/2019/04/14/decisionStump.html","headline":"A succinct solution for building decision stumps","dateModified":"2019-04-14T00:00:00+09:30","datePublished":"2019-04-14T00:00:00+09:30","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/04/14/decisionStump.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
  </head>
  <body>

    <a id="skip-to-content" href="#content">Skip to the content.</a>
    <header class="page-header" role="banner">

      <h1 class="project-name">A succinct solution for building decision stumps</h1>
      <h2 class="project-tagline"></h2>

        
        <ul class="menu">
          <li class="menu_li"><a href="https://www.owenll66.com/">Blog</a></li>
          <li class="menu_li"><a href="https://www.owenll66.com/project.html">Project</a></li>
          <li class="menu_li"><a href="https://www.owenll66.com/about.html">About</a></li>
          <li class="menu_li"><a href="https://www.owenll66.com/fun.html">Fun</a></li>
        </ul>
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="a-succinct-solution-for-building-decision-stumps">A succinct solution for building decision stumps</h1>

<h2 id="background">Background</h2>

<p>I encountered this code when I was studying in the university on a machine
learning course. I was intrigued by the succinct and concise code which was
provided by my professor and I personally translated it in to python. It applies
some mathematics and may look scary to understand at the first sight. However,
you will find out the elegance of the code after you comprehend it.</p>

<h2 id="why-decision-stump-for-boosting-algorithms">Why decision stump for boosting algorithms?</h2>
<p>When it comes to boosting algorithms, we have to talks about weak learners. Weak
learners are algorithms in classification which can achieve slightly better than
50% accuracy. Boosting algorithms basically combine those weak learners and
train them by using training data to learn the appropriate weights of each weak
learner, to produce a strong classifier with high accuracy. A decision stump is
a decision tree with only one depth (Please refer to decision tree algorithm).
Due to it is a very simple weak learner and unlikely to occur overfitting, I
would say it is a safe option to choose as weak classifier and the performance
is usually good. Nonetheless, of course, you can choose other classifiers as
weak learners and test them out.</p>

<h2 id="what-if-the-prediction-accuracy-is-less-than-50">What if the prediction accuracy is less than 50%</h2>
<p>In binary case: Easy! Just need to flip the labels from one to another. As you
will see in the code.</p>

<h2 id="matlab">Matlab</h2>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% build a stump from each component and return the best</span>
<span class="k">function</span> <span class="p">[</span><span class="n">stump</span><span class="p">]</span> <span class="o">=</span> <span class="n">build_stump</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">w</span><span class="p">);</span> <span class="c1">% normalized the weights (if not already)</span>

<span class="n">stump</span> <span class="o">=</span> <span class="nb">cell</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
<span class="n">werr</span> <span class="o">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
<span class="k">for</span> <span class="nb">i</span><span class="o">=</span><span class="mi">1</span><span class="p">:</span><span class="n">d</span><span class="p">,</span>
  <span class="n">stump</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span> <span class="o">=</span> <span class="n">build_onedim_stump</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="nb">i</span><span class="p">),</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">);</span>
  <span class="n">stump</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span><span class="o">.</span><span class="n">ind</span> <span class="o">=</span> <span class="nb">i</span><span class="p">;</span>
  <span class="n">werr</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="o">=</span> <span class="n">stump</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span><span class="o">.</span><span class="n">werr</span><span class="p">;</span>
<span class="k">end</span><span class="p">;</span>

<span class="p">[</span><span class="n">min_werr</span><span class="p">,</span><span class="n">ind</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">werr</span><span class="p">);</span>

<span class="n">stump</span> <span class="o">=</span> <span class="n">stump</span><span class="p">{</span><span class="n">ind</span><span class="p">(</span><span class="mi">1</span><span class="p">)};</span> <span class="c1">% return the best stump</span>


<span class="c1">% build a stump from a single input component</span>

<span class="k">function</span> <span class="p">[</span><span class="n">stump</span><span class="p">]</span> <span class="o">=</span> <span class="n">build_onedim_stump</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>

<span class="p">[</span><span class="n">xsorted</span><span class="p">,</span><span class="n">I</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sort</span><span class="p">(</span><span class="n">x</span><span class="p">);</span> <span class="c1">% ascending</span>
<span class="n">Ir</span> <span class="o">=</span> <span class="n">I</span><span class="p">(</span><span class="k">end</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">);</span> <span class="c1">% descending</span>

<span class="n">score_left</span>  <span class="o">=</span> <span class="nb">cumsum</span><span class="p">(</span><span class="n">w</span><span class="p">(</span><span class="n">I</span><span class="p">)</span><span class="o">.*</span><span class="n">y</span><span class="p">(</span><span class="n">I</span><span class="p">));</span> <span class="c1">% left to right sums</span>
<span class="n">score_right</span> <span class="o">=</span> <span class="nb">cumsum</span><span class="p">(</span><span class="n">w</span><span class="p">(</span><span class="n">Ir</span><span class="p">)</span><span class="o">.*</span><span class="n">y</span><span class="p">(</span><span class="n">Ir</span><span class="p">));</span>  <span class="c1">% right to left sums</span>

<span class="c1">% score the -1 -&gt; 1 boundary between successive points</span>
<span class="n">score</span> <span class="o">=</span> <span class="o">-</span><span class="n">score_left</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">score_right</span><span class="p">(</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">);</span>

<span class="c1">% find distinguishable points (possible boundary locations)</span>

<span class="n">Idec</span> <span class="o">=</span> <span class="nb">find</span><span class="p">(</span> <span class="n">xsorted</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">&lt;</span><span class="n">xsorted</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="p">);</span>

<span class="c1">% locate the boundary or give up</span>

<span class="k">if</span> <span class="p">(</span><span class="nb">length</span><span class="p">(</span><span class="n">Idec</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">),</span>
  <span class="p">[</span><span class="n">maxscore</span><span class="p">,</span><span class="n">ind</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span> <span class="nb">abs</span><span class="p">(</span><span class="n">score</span><span class="p">(</span><span class="n">Idec</span><span class="p">))</span> <span class="p">);</span> <span class="c1">% maximum weighted agreement</span>
  <span class="n">ind</span> <span class="o">=</span> <span class="n">Idec</span><span class="p">(</span><span class="n">ind</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>

  <span class="n">stump</span><span class="o">.</span><span class="n">werr</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">maxscore</span><span class="p">;</span> <span class="c1">% weighted error</span>
  <span class="n">stump</span><span class="o">.</span><span class="n">x0</span>   <span class="o">=</span> <span class="p">(</span><span class="n">xsorted</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span><span class="o">+</span><span class="n">xsorted</span><span class="p">(</span><span class="n">ind</span><span class="o">+</span><span class="mi">1</span><span class="p">))/</span><span class="mi">2</span><span class="p">;</span> <span class="c1">% threshold</span>
  <span class="n">stump</span><span class="o">.</span><span class="n">s</span>    <span class="o">=</span> <span class="nb">sign</span><span class="p">(</span><span class="n">score</span><span class="p">(</span><span class="n">ind</span><span class="p">));</span> <span class="c1">% direction of -1 -&gt; 1 change</span>
<span class="k">else</span>
  <span class="n">stump</span><span class="o">.</span><span class="n">werr</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">;</span>
  <span class="n">stump</span><span class="o">.</span><span class="n">x0</span>   <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">stump</span><span class="o">.</span><span class="n">s</span>    <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="k">end</span><span class="p">;</span>
</code></pre></div></div>
<h2 id="code-essence">Code essence</h2>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">score_left</span>  <span class="o">=</span> <span class="nb">cumsum</span><span class="p">(</span><span class="n">w</span><span class="p">(</span><span class="n">I</span><span class="p">)</span><span class="o">.*</span><span class="n">y</span><span class="p">(</span><span class="n">I</span><span class="p">));</span> <span class="c1">% left to right sums</span>
<span class="n">score_right</span> <span class="o">=</span> <span class="nb">cumsum</span><span class="p">(</span><span class="n">w</span><span class="p">(</span><span class="n">Ir</span><span class="p">)</span><span class="o">.*</span><span class="n">y</span><span class="p">(</span><span class="n">Ir</span><span class="p">));</span>  <span class="c1">% right to left sums</span>

<span class="c1">% score the -1 -&gt; 1 boundary between successive points</span>
<span class="n">score</span> <span class="o">=</span> <span class="o">-</span><span class="n">score_left</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">score_right</span><span class="p">(</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>

<p>Explanation:
“score_left” sums up all the weighted labels on the left of all the possible
splits and “score_right” sums up all the weighted labels on the right of all the
possible splits. And “score” is an array of “information gain” calculated for
each split. Using “score” we can find the highest “information gain” which
splits the data in order to have one type of weighted label (“+” or “-“) value
as higher as possible on each side.</p>

<h2 id="python3">Python3</h2>
<p>Here is the code with the same logic that I translated into Python</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#tree stump</span>
<span class="k">class</span> <span class="nc">Stump</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weighted_error</span><span class="p">,</span> <span class="n">split</span><span class="p">,</span> <span class="n">dimension</span><span class="p">,</span> <span class="n">sign</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_error</span> <span class="o">=</span> <span class="n">weighted_error</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dimension</span> <span class="o">=</span> <span class="n">dimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sign</span> <span class="o">=</span> <span class="n">sign</span>

<span class="c">#####################################################################</span>
<span class="c">#Build the Decision Tree Stump</span>
<span class="c">#INPUT:    X_train -- the training features</span>
<span class="c">#          y_train -- the training labels</span>
<span class="c">#          w -- weight of each sample</span>
<span class="c">#OUTPUT:   dtStump[index] -- the best split stump</span>
<span class="c">#####################################################################</span>
<span class="k">def</span> <span class="nf">build_DTStump</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
    <span class="n">dimension</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">dtStumps</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">weighted_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dimension</span><span class="p">)</span>
    <span class="c">#initialise decision tree stumps for each dimension</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dimension</span><span class="p">):</span>
        <span class="n">dtStumps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">build_onedim_stump</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span><span class="n">y_train</span><span class="p">))</span>
        <span class="n">dtStumps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dimension</span> <span class="o">=</span> <span class="n">i</span>
        <span class="n">weighted_error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">dtStumps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weighted_error</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">weighted_error</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dtStumps</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

<span class="c">#####################################################################</span>
<span class="c">#Build one dimensional Decision Tree Stump</span>
<span class="c">#INPUT:    X_train -- the training features</span>
<span class="c">#          y_train -- the training labels</span>
<span class="c">#          w -- weight of each sample</span>
<span class="c">#OUTPUT:   stump -- the best split stump in this dimension</span>
<span class="c">#####################################################################</span>
<span class="k">def</span> <span class="nf">build_onedim_stump</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">stump</span> <span class="o">=</span> <span class="n">Stump</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">)</span>

    <span class="n">sortedx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c">#indices in ascending order</span>
    <span class="n">I</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="c">#indices in descending order</span>
    <span class="n">Ir</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c">#left to right sums</span>
    <span class="n">score_left</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">I</span><span class="p">])</span>
    <span class="c">#right to left sums</span>
    <span class="n">score_right</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">Ir</span><span class="p">])</span>

    <span class="n">score</span> <span class="o">=</span> <span class="o">-</span><span class="n">score_left</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">score_right</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">sortedx</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">sortedx</span><span class="p">[</span><span class="mi">1</span><span class="p">:])[:,</span><span class="mi">0</span><span class="p">]</span>

    <span class="c">#locate the boundary</span>
    <span class="k">if</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">maxscore</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">score</span><span class="p">[</span><span class="n">indices</span><span class="p">]))</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">score</span><span class="p">[</span><span class="n">indices</span><span class="p">]))</span>
        <span class="n">index</span>  <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

        <span class="n">stump</span><span class="o">.</span><span class="n">weighted_error</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">maxscore</span>
        <span class="c">#get the split value</span>
        <span class="n">stump</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="p">(</span><span class="n">sortedx</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">+</span><span class="n">sortedx</span><span class="p">[</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">stump</span><span class="o">.</span><span class="n">sign</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">score</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">stump</span><span class="o">.</span><span class="n">weighted_error</span> <span class="o">=</span> <span class="mf">0.5</span>
        <span class="n">stump</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">stump</span><span class="o">.</span><span class="n">sign</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">stump</span>
</code></pre></div></div>
<h2 id="discussion">Discussion</h2>
<p>If using decision stump as weak learner, will the classifier still overfit?<br />
The answer is YES! This blog is not going to talk about the mathematical
mechanism behind this. But it is proven that if there are too many iterations on
training, overfitting will still occur.
<br /></p>

<p>Please leave a comment if you have any questions or insights about this blog. Or
if you would like to help construct this website, please Email
<em>owen.liu_owen@qq.com</em>.</p>

<p><br /></p>

<hr />


      <!-- Disqus Board -->
      <div id="disqus_thread"></div>
      <script type="text/javascript">
          /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
          var disqus_shortname = 'owenll66'; // required: replace example with your forum shortname
          /* * * DON'T EDIT BELOW THIS LINE * * */
          (function() {
              var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
              dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
              (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
      </script>
      <noscript>Please enable JavaScript to view <a href="https://disqus.com/?ref_noscript">MewX comments powered by Disqus.</a></noscript>

      <footer class="site-footer">
        
        <span class="site-footer-credits">Ⓒ 2019-2019 Owenll66 |
          powered by <a href="https://jekyllrb.com/">Jekyll</a>,
          <a href="http://jekyllthemes.org/themes/cayman-theme/">Cayman Theme</a>
          and <a href="https://pages.github.com/">Github Pages</a>
        </span>
      </footer>
    </main>
  </body>
</html>
